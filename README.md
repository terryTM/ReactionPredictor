# ReactionPredictor

Predicting chemical reactions has various scientific in- dustrial, and practical applications. Current chemical reaction prediction is done by searching vast realms of scientific literature to find experimental results from a specific reac- tion. We implemented a natural language deep learning based approach to predict chemical reactions. By examining several new tokenization and input encoding schemes for a dataset of chemical reactions in string form and training a transformer model, we were able to obtain reasonably accurate predictors of chemical reactions and demonstrate an improvement over simpler methods of tokenization.

We defined a sequence-to-sequence architecture using a transformer model. We expected the transformer to perform better than previous LSTM-based encoder-decoders due to the self-attention mechanism of the transformer. This mechanism allows transformers to learn dependencies across entire SMILES strings, which is especially important as reactions can modify any part of a string. 

We evaluated the Transformer model on several tokenization and input encoding schemes: atom-wise tokenization, AIS tokenization, and functional group encoding. To evaluate the prediction quality of our models, we performed a greedy search procedure to generate translations from reactant strings to product strings. To qualify as a correct prediction, predicted product strings needed to be exact matches to target strings.

Our results show a maximum accuracy of 76.1% achieved using atom-wise tokenization on the UPSTO test set. Performance was worse on the OCR test set, with 53.8% accuracy. When using AIS tokenization, model performance was degraded, decreasing accuracy to 73.6%. AIS performance on the OCR test set was 49.5%. We observe similar loss curves (Figure 3) for both tokeniza- tion schemes and almost identical validation losses after 50 epochs. Training loss, however, is lower for the AIS model. It is likely that the AIS model has a greater degree of over- fitting to the training data, resulting in a degraded model performance compared to the atom-wise model. We also noted that around 2.7% of predictions were invalid SMILES strings.
